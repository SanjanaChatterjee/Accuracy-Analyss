import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Load refined dataset
df = pd.read_csv("cars_refined.csv")

# -----------------------------
# Feature Engineering
# -----------------------------

# Create "Car_Age" feature from Year
df["Car_Age"] = 2025 - df["Year"]

# Encode categorical variables
cat_cols = ["Brand", "Model", "Fuel_Type", "Transmission", "Owner_Type"]
le = LabelEncoder()
for col in cat_cols:
    df[col] = le.fit_transform(df[col])

# Drop unnecessary columns
if "Car_ID" in df.columns:
    df.drop("Car_ID", axis=1, inplace=True)

# -----------------------------
# Train-Test Split
# -----------------------------
X = df.drop("Price", axis=1)
y = df["Price"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# -----------------------------
# Model Building
# -----------------------------
model = RandomForestRegressor(n_estimators=200, random_state=42)
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

# -----------------------------
# Evaluation
# -----------------------------
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print("Model Performance:")
print(f"MAE  : {mae:.2f}")
print(f"RMSE : {rmse:.2f}")
print(f"R2   : {r2:.2f}")

# -----------------------------
# Feature Importance
# -----------------------------
import matplotlib.pyplot as plt
import seaborn as sns

feat_importances = pd.Series(model.feature_importances_, index=X.columns)
plt.figure(figsize=(10,6))
sns.barplot(x=feat_importances, y=feat_importances.index)
plt.title("Feature Importance in Car Price Prediction")
plt.show()
